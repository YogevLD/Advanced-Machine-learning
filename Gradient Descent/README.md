## Gradient Descent Project
### Overview
This project is an independent implementation of the gradient descent algorithm, including its two main variants: mini-batch and stochastic (random) gradient descent. The goal of this project is to provide a clear, from-scratch demonstration of how gradient descent can be used to optimize functions, with a focus on machine learning applications.

### Features
1. Gradient Descent Implementation: Core implementation of the gradient descent algorithm to find the minimum of a function.
2. Mini-Batch Gradient Descent: Implementation of the mini-batch approach, which is a compromise between the full dataset iteration and the stochastic method.
3. Stochastic Gradient Descent: Implementation of the stochastic (or random) gradient descent, highlighting its efficiency on large datasets.
Results Explanation: Detailed explanations of the results obtained from each method, including convergence rates and comparison of performance.
